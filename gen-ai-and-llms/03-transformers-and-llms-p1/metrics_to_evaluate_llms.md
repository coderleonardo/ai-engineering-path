
Evaluates how well the probability predicted by the model aligns with the actual distribution of the data.
The lower, the better.

## Accuracy

Proportion of correct predictions made by the model in relation to the total. Used in classification tasks.

$$\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$$

## Precision and Recall

Precision is the proportion of all positive classifications made by the model that are actually positive.

$$\text{Precision} = \frac{TP}{TP + FP}$$

Recall or true positive rate is the proportion of all actual positives that were correctly classified as positive.

$$\text{Recall} = \frac{TP}{TP + FN}$$

## F1-Score

Harmonic combination between precision and recall.

$$\text{F1-Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$

## ROC-AUC and PR-AUC

Commonly used in classification tasks, with PR-AUC indicated for imbalanced data.

## BLEU (BiLingual Evaluation Understudy)

Used to evaluate the quality of generated texts by comparing the generated text with a reference text.

## Other Metrics

- Token Cost

- Word Error Rate (WER): evaluates the difference between a sequence generated by a system and a reference sequence

---

For more metrics see https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall?hl=pt-br

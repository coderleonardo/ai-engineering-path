## BERT (Bidirectional Encoder Representations from Transformers)

Modelos bidirecional que usa a informação tanto à esquerda quanto à direita de uma palavra numa sequência para executar determinada tarefa.

Útil para problemas como classificação de sentimentos e extração de entiddades nomeadas.

Explicação mais completa: https://huggingface.co/blog/bert-101

## GPT (Generative Pre-Trained Transformer)

Focado em geração de texto o GPT usa o contexto à esquerda para prever a próxima palavra numa sequência.

Útil em problemas de geração de texto. 

Reporte técnico sobre o GPT-4: https://cdn.openai.com/papers/gpt-4.pdf
## BERT (Bidirectional Encoder Representations from Transformers)

Bidirectional model that uses information both to the left and right of a word in a sequence to perform a specific task.

Useful for problems such as sentiment classification and named entity extraction.

More detailed explanation: https://huggingface.co/blog/bert-101

## GPT (Generative Pre-Trained Transformer)

Focused on text generation, GPT uses the context to the left to predict the next word in a sequence.

Useful for text generation problems.

Technical report on GPT-4: https://cdn.openai.com/papers/gpt-4.pdf